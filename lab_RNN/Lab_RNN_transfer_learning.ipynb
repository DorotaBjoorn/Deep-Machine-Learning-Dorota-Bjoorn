{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - transfer learning\n",
    "\n",
    "Own exploration of tranfer learning for classification tasks.\\\n",
    "Content inspired by https://www.geeksforgeeks.org/fine-tuning-bert-model-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dorota/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dorota/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-01-28 11:12:58.863735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-28 11:12:58.863820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-28 11:12:58.865097: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-28 11:12:58.874479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-28 11:13:01.135020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# preprocess text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# #for transformers\n",
    "# import transformers\n",
    "# from transformers import BertTokenizerFast, BertForSequenceClassification, BertModel\n",
    "\n",
    "# # deep ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# # from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_df(text):\n",
    "    '''Count total number of words in dataframe'''\n",
    "    rows = text.apply(lambda row: row.split(' ')) # create a list of words for each row\n",
    "    word_count = [word for row in rows for word in row] # concatinate all words from whole df into one list\n",
    "    word_uniqe = set(word_count)\n",
    "    return len(word_count), len(word_uniqe)\n",
    "\n",
    "\n",
    "def remove_stop_words_and_lemmatize(text, lemmatizer):\n",
    "    '''Removes stop words and lemmatizes text (nouns and verbs)'''\n",
    "    words = text.split(' ')\n",
    "    words_cleaned = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))] # lemmatize nouns\n",
    "    words_cleaned = [lemmatizer.lemmatize(word, pos='v') for word in words_cleaned] # lemmatize verbs\n",
    "    text_cleaned = (' ').join(words_cleaned)\n",
    "    return text_cleaned\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies, test_loss, test_accuracy):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(val_losses, label='val')\n",
    "    plt.title(f'Loss (test loss = {test_loss:.4f})')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='train')\n",
    "    plt.plot(val_accuracies, label='val')\n",
    "    plt.title(f'Accuracy (test accuracy = {test_accuracy:.4f})')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_confusion_matrix(y_pred, y_test, labels_dict):\n",
    "    labels_list = list(labels_dict.keys())\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=labels_list, yticklabels=labels_list)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and data pre- processing\n",
    "* lemmatize\n",
    "* remove stop words\n",
    "* one-hot encode labels\n",
    "* train|val|test split\n",
    "\n",
    "*For EDA of the data see Lab_RNN_base*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/emotions.csv')\n",
    "df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df.text = df.text.apply(lambda row_text: remove_stop_words_and_lemmatize(row_text, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"love\": 3, \"sadness\": 4, \"surprise\": 5}\n",
    "\n",
    "df.label = df.label.replace(labels_dict) # str -> int\n",
    "y_array = df.label.to_numpy() # pd.Series -> array\n",
    "y_array = to_categorical(y_array, len(labels_dict)) # one-hot encoding NOTE: no one-hot encoding for Huggingface - transformers Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = df.text.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000,), (1000,), (1000,), (3000, 6), (1000, 6), (1000, 6))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainval, X_test, y_trainval, y_test =train_test_split(X_array, y_array, test_size=0.20, random_state=42)\n",
    "X_train, X_val, y_train, y_val =train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Huggingface - transformers Trainer\n",
    "https://medium.com/grabngoinfo/transfer-learning-for-text-classification-using-hugging-face-transformers-trainer-13407187cf89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, TextClassificationPipeline\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import evaluate #!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data into dataset to be able to handle larger amounts of data\n",
    "train_dataset = Dataset.from_dict({'text': X_train, 'label': y_train}) # NOTE y data is not to be one-hot encoded\n",
    "val_dataset = Dataset.from_dict({'text': X_val, 'label': y_val})\n",
    "test_dataset = Dataset.from_dict({'text': X_test, 'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 3000\n",
       " }),\n",
       " {'text': 'feel rude say better men', 'label': 0})"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'], dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between special tokens and their IDs.\n",
    "print(f'The unknown token is {tokenizer.unk_token} and the ID for the unkown token is {tokenizer.unk_token_id}.')\n",
    "print(f'The seperator token is {tokenizer.sep_token} and the ID for the seperator token is {tokenizer.sep_token_id}.')\n",
    "print(f'The pad token is {tokenizer.pad_token} and the ID for the pad token is {tokenizer.pad_token_id}.')\n",
    "print(f'The sentence level classification token is {tokenizer.cls_token} and the ID for the classification token is {tokenizer.cls_token_id}.')\n",
    "print(f'The mask token is {tokenizer.mask_token} and the ID for the mask token is {tokenizer.mask_token_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data[\"text\"], max_length=35, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 2483.80 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2236.38 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2681.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the train, val and test datasets\n",
    "train_dataset_tokenized = dataset['train'].map(tokenize_dataset)\n",
    "val_dataset_tokenized = dataset['validation'].map(tokenize_dataset)\n",
    "test_dataset_tokenized = dataset['test'].map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 3000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " }))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_tokenized, val_dataset_tokenized, test_dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_hugging = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./transfer_learning_transformer/\",          \n",
    "    logging_dir='./transfer_learning_transformer/logs',            \n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,    \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=32,  \n",
    "    learning_rate=5e-6,\n",
    "    seed=42,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all evaluation metrics/models in Hugging Face\n",
    "# evaluate.list_evaluation_modules()\n",
    "\n",
    "# Function to compute the metric\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    # probabilities = tf.nn.softmax(logits)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    model=model_hugging,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=val_dataset_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 36:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.566400</td>\n",
       "      <td>1.523262</td>\n",
       "      <td>0.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.436300</td>\n",
       "      <td>1.367639</td>\n",
       "      <td>0.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.315800</td>\n",
       "      <td>1.311843</td>\n",
       "      <td>0.531000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=282, training_loss=1.4395047992679244, metrics={'train_runtime': 2223.1567, 'train_samples_per_second': 4.048, 'train_steps_per_second': 0.127, 'total_flos': 161880779340000.0, 'train_loss': 1.4395047992679244, 'epoch': 3.0})"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictions\n",
    "y_test_predict = trainer.predict(test_dataset_tokenized)\n",
    "#print(y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3267856 , -0.44103605,  1.7728944 ,  0.03134485,  0.06998684,\n",
       "        -1.0767158 ],\n",
       "       [-0.2656088 , -0.45882598,  0.9377078 , -0.3954832 ,  0.8267701 ,\n",
       "        -1.7086649 ],\n",
       "       [-0.37199643, -0.34076005,  1.7501726 ,  0.31945994, -0.04927007,\n",
       "        -1.2711248 ]], dtype=float32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict.predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[0.07437343, 0.06634367, 0.60715175, 0.10640252, 0.11059459,\n",
       "        0.03513397],\n",
       "       [0.10809388, 0.0891021 , 0.36007655, 0.09492867, 0.32226655,\n",
       "        0.02553229],\n",
       "       [0.07059461, 0.07283453, 0.5894104 , 0.14095068, 0.0974832 ,\n",
       "        0.02872665]], dtype=float32)>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probabilities\n",
    "y_test_probabilities = tf.nn.softmax(y_test_predict.predictions)\n",
    "y_test_probabilities[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted labels\n",
    "y_test_pred_labels = np.argmax(y_test_probabilities, axis=1)\n",
    "y_test_pred_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 2, 4, 4, 3, 4, 4, 5, 2, 3, 4, 0, 1, 4, 4, 0, 2, 2])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual labels\n",
    "y_test_actual_labels = y_test_predict.label_ids\n",
    "y_test_actual_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2866343259811401,\n",
       " 'eval_accuracy': 0.555,\n",
       " 'eval_runtime': 66.046,\n",
       " 'eval_samples_per_second': 15.141,\n",
       " 'eval_steps_per_second': 0.485,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "trainer.evaluate(test_dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.555}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and compute f1 metric\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_f1.compute(predictions=y_test_pred_labels, references=y_test_actual_labels, average='weighted')\n",
    "\n",
    "# Load and compute recall metric\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "metric_recall.compute(predictions=y_test_pred_labels, references=y_test_actual_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save tokenizer and model\n",
    "# tokenizer.save_pretrained('./transfer_learning_transformer/')\n",
    "# trainer.save_model('./transfer_learning_transformer/')\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./transfer_learning_transformer/\")\n",
    "# loaded_model = AutoModelForSequenceClassification.from_pretrained('./transfer_learning_transformer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face\n",
    "https://huggingface.co/docs/transformers/training\\\n",
    "https://huggingface.co/learn/nlp-course/chapter3/1?fw=tf\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(100, 33), dtype=int32, numpy=\n",
       "array([[  101,  2514, 18138, ...,     0,     0,     0],\n",
       "       [  101,  4921,  2063, ...,     0,     0,     0],\n",
       "       [  101,  2036,  2514, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  5674,  2619, ...,     0,     0,     0],\n",
       "       [  101,  4921,  2063, ...,     0,     0,     0],\n",
       "       [  101,  3685,  2514, ...,     0,     0,     0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(100, 33), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(100, 33), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model_inputs = tokenizer(X_array.tolist(), padding='longest', truncation=True, return_tensors=\"tf\") # tensor flow tensor returned\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3.77859890e-01  3.31305638e-02 -4.14384872e-01  2.68686920e-01\n",
      "  -2.25081965e-01 -3.92402738e-01]\n",
      " [ 1.47919461e-01 -9.85136181e-02 -7.66087174e-02  5.12365937e-01\n",
      "  -3.03294241e-01 -3.16262960e-01]\n",
      " [ 1.58609882e-01 -1.00193717e-01 -1.99917838e-01  4.08303022e-01\n",
      "  -2.44259611e-01 -3.46224546e-01]\n",
      " [ 2.42585778e-01  2.58706883e-03 -3.20779413e-01  2.95459986e-01\n",
      "  -2.36818656e-01 -3.80875170e-01]\n",
      " [ 2.74985135e-01  2.92812437e-02 -3.78858566e-01  3.66466701e-01\n",
      "  -2.82129407e-01 -3.62889767e-01]\n",
      " [ 2.56998003e-01 -2.88348608e-02 -2.88001716e-01  4.25435901e-01\n",
      "  -1.65954813e-01 -3.19993556e-01]\n",
      " [ 1.65628806e-01 -1.00769818e-01 -1.51799291e-01  4.70328122e-01\n",
      "  -3.11065435e-01 -2.85241604e-01]\n",
      " [ 2.19614893e-01 -1.32952351e-02 -2.22404927e-01  3.30655545e-01\n",
      "  -1.78189039e-01 -3.96509230e-01]\n",
      " [ 4.01930720e-01  6.27020895e-02 -4.48277056e-01  1.74111009e-01\n",
      "  -1.85422987e-01 -4.34740454e-01]\n",
      " [ 7.03607053e-02 -3.06887925e-02 -5.29971719e-03  5.84378242e-01\n",
      "  -3.82330775e-01 -2.37294137e-01]\n",
      " [ 1.01425365e-01 -3.72191146e-02 -4.12398726e-02  5.96540570e-01\n",
      "  -4.12253797e-01 -2.31552675e-01]\n",
      " [ 1.17997184e-01 -7.08849505e-02 -4.04702723e-02  5.22494316e-01\n",
      "  -3.89525682e-01 -2.39491999e-01]\n",
      " [ 1.23434752e-01 -1.47647589e-01 -1.45583689e-01  4.63701367e-01\n",
      "  -3.41023028e-01 -2.29811922e-01]\n",
      " [ 2.90899336e-01  1.93418302e-02 -3.51378649e-01  2.84924179e-01\n",
      "  -2.14378685e-01 -4.36606288e-01]\n",
      " [ 4.83597964e-02 -5.09083942e-02 -6.28214851e-02  6.42351210e-01\n",
      "  -4.55734313e-01 -2.72362411e-01]\n",
      " [ 2.24725515e-01 -5.77926673e-02 -2.47027054e-01  3.59510154e-01\n",
      "  -2.65563756e-01 -3.62814128e-01]\n",
      " [ 1.75127178e-01 -3.06152217e-02 -1.74985111e-01  4.70651895e-01\n",
      "  -2.85594404e-01 -3.36605132e-01]\n",
      " [ 5.62369823e-02 -8.94724578e-02 -7.01972842e-03  5.72211921e-01\n",
      "  -4.05319780e-01 -2.08931267e-01]\n",
      " [ 1.95726439e-01  1.04733016e-02 -2.64439940e-01  3.89550835e-01\n",
      "  -3.20690691e-01 -3.38581473e-01]\n",
      " [-3.31744105e-02 -1.43686607e-01 -7.90984929e-03  5.86010456e-01\n",
      "  -3.83837849e-01 -2.19329610e-01]\n",
      " [ 2.98750520e-01  6.31174296e-02 -3.61217320e-01  3.07250082e-01\n",
      "  -2.69058585e-01 -4.38010633e-01]\n",
      " [-9.20704901e-02 -1.70298159e-01  7.02853426e-02  6.62765026e-01\n",
      "  -3.38003516e-01 -2.39356592e-01]\n",
      " [ 1.63192064e-01 -1.04823537e-01 -2.48676091e-01  4.50135797e-01\n",
      "  -3.02312791e-01 -2.40499392e-01]\n",
      " [ 1.72117144e-01 -8.70600864e-02 -1.78391382e-01  5.33694625e-01\n",
      "  -2.90040940e-01 -3.33098173e-01]\n",
      " [ 1.43948808e-01 -3.83736193e-03 -1.04337990e-01  4.72388536e-01\n",
      "  -2.92728454e-01 -3.23356509e-01]\n",
      " [ 1.49310082e-01 -5.09806573e-02 -1.68939844e-01  4.72769320e-01\n",
      "  -2.27651536e-01 -2.52405554e-01]\n",
      " [ 1.88069001e-01 -5.50562218e-02 -1.48360878e-01  5.22565603e-01\n",
      "  -3.59297812e-01 -2.45215029e-01]\n",
      " [ 1.93715617e-02 -8.00787210e-02 -1.65805072e-02  6.54889166e-01\n",
      "  -3.77110928e-01 -2.21566364e-01]\n",
      " [ 3.57161969e-01 -2.58404389e-02 -3.83750021e-01  2.17555851e-01\n",
      "  -1.33683324e-01 -4.02319014e-01]\n",
      " [ 2.42964178e-01 -5.15988059e-02 -2.04030767e-01  4.00172800e-01\n",
      "  -2.60238260e-01 -3.57225001e-01]\n",
      " [ 3.91786397e-01  4.16919403e-02 -4.75096732e-01  2.37728968e-01\n",
      "  -2.12605208e-01 -4.47510332e-01]\n",
      " [ 3.44109654e-01  4.79085632e-02 -4.18721765e-01  3.12716365e-01\n",
      "  -2.75250137e-01 -4.17052269e-01]\n",
      " [ 3.72248650e-01  1.17811173e-01 -4.12509203e-01  3.33488286e-01\n",
      "  -2.10514739e-01 -3.53461325e-01]\n",
      " [ 3.86510491e-01  6.99222535e-02 -4.36232597e-01  2.62533844e-01\n",
      "  -2.00800985e-01 -4.05602425e-01]\n",
      " [ 2.83416688e-01 -2.71330550e-02 -2.62155533e-01  3.36903214e-01\n",
      "  -3.10629040e-01 -2.98589915e-01]\n",
      " [ 1.92525148e-01 -7.66541511e-02 -2.22758695e-01  3.85780931e-01\n",
      "  -2.52403408e-01 -3.34061742e-01]\n",
      " [ 2.63872802e-01  5.10715973e-03 -3.18461239e-01  3.23811054e-01\n",
      "  -2.49753803e-01 -3.10321927e-01]\n",
      " [ 3.57864380e-01  2.53346972e-02 -3.96998286e-01  3.08283985e-01\n",
      "  -2.25914538e-01 -3.79234195e-01]\n",
      " [ 7.00981095e-02 -8.19076300e-02 -1.27643347e-04  5.69267869e-01\n",
      "  -3.50798368e-01 -2.35192508e-01]\n",
      " [-9.67224538e-02 -1.56619608e-01  9.63520035e-02  6.47213042e-01\n",
      "  -4.43627834e-01 -1.75220251e-01]\n",
      " [ 8.59107152e-02 -6.88832924e-02 -6.24930263e-02  5.18828511e-01\n",
      "  -4.33008254e-01 -1.91391855e-01]\n",
      " [ 1.87312320e-01 -7.52793849e-02 -1.74476534e-01  4.63771820e-01\n",
      "  -3.17136139e-01 -3.08955312e-01]\n",
      " [ 3.26264739e-01 -9.09260660e-03 -3.35409135e-01  3.25837761e-01\n",
      "  -2.49043390e-01 -4.05448020e-01]\n",
      " [ 2.26735160e-01 -5.78619651e-02 -1.97408140e-01  4.04836863e-01\n",
      "  -2.44646221e-01 -2.77576506e-01]\n",
      " [ 2.83266455e-01 -4.07587364e-03 -2.95084715e-01  3.93488288e-01\n",
      "  -2.29940340e-01 -3.44297826e-01]\n",
      " [ 1.50709897e-01 -7.27610886e-02 -6.21721148e-02  4.98214364e-01\n",
      "  -3.23907137e-01 -2.61977434e-01]\n",
      " [ 4.18968081e-01  5.94605803e-02 -5.23937583e-01  1.11637183e-01\n",
      "  -2.61813402e-01 -4.22201633e-01]\n",
      " [ 2.04199195e-01 -4.57752198e-02 -1.62585393e-01  5.27185023e-01\n",
      "  -3.67296338e-01 -3.06686521e-01]\n",
      " [ 3.32506776e-01  3.14507261e-02 -4.19258416e-01  2.01299071e-01\n",
      "  -1.98276818e-01 -4.29103911e-01]\n",
      " [ 3.15564215e-01 -5.12496196e-03 -2.67379165e-01  3.14768821e-01\n",
      "  -1.95014104e-01 -3.89982164e-01]\n",
      " [ 3.72422814e-01  5.55700064e-03 -4.27046925e-01  2.39552379e-01\n",
      "  -2.26604864e-01 -4.16365951e-01]\n",
      " [ 1.86414838e-01  1.53414421e-02 -2.06039369e-01  5.00595212e-01\n",
      "  -3.21008563e-01 -2.71187305e-01]\n",
      " [ 4.16657686e-01  2.16937140e-02 -5.19405782e-01  1.68481559e-01\n",
      "  -2.24985063e-01 -4.25312221e-01]\n",
      " [ 3.16064119e-01  4.76022512e-02 -3.78493786e-01  2.96297193e-01\n",
      "  -2.56901026e-01 -3.60901982e-01]\n",
      " [-5.21712601e-02 -1.52723208e-01  7.25814775e-02  5.58160603e-01\n",
      "  -3.69455040e-01 -2.09278017e-01]\n",
      " [ 3.67254078e-01  4.43738438e-02 -4.57869649e-01  2.16365889e-01\n",
      "  -2.01966718e-01 -3.90534997e-01]\n",
      " [ 1.86898082e-01 -7.42259100e-02 -1.64073020e-01  4.11602378e-01\n",
      "  -3.56929570e-01 -2.47608542e-01]\n",
      " [ 1.76649034e-01 -6.84269220e-02 -1.87396467e-01  4.62246776e-01\n",
      "  -3.38931143e-01 -3.36556077e-01]\n",
      " [-1.20972693e-02 -1.02840364e-01 -2.47686356e-02  5.77427208e-01\n",
      "  -3.99277747e-01 -2.59997427e-01]\n",
      " [ 2.77574986e-01  2.34707408e-02 -3.09887230e-01  3.10012758e-01\n",
      "  -2.75021195e-01 -3.67447972e-01]\n",
      " [ 3.44415426e-01 -6.36932850e-02 -4.01604146e-01  1.99350119e-01\n",
      "  -1.63488895e-01 -4.26531881e-01]\n",
      " [ 7.21362233e-02 -3.74144018e-02 -4.87263799e-02  5.82202256e-01\n",
      "  -4.24770623e-01 -2.02377796e-01]\n",
      " [ 3.48468363e-01  6.96275085e-02 -4.04928058e-01  3.16482276e-01\n",
      "  -2.14920983e-01 -3.64588439e-01]\n",
      " [ 1.56381473e-01 -3.64395007e-02 -1.22568130e-01  5.11234939e-01\n",
      "  -3.78831476e-01 -2.32443839e-01]\n",
      " [ 3.53482366e-01  3.20321657e-02 -3.69859159e-01  3.44160855e-01\n",
      "  -2.21479461e-01 -3.82189989e-01]\n",
      " [ 2.47239247e-01  2.53764819e-02 -3.05514634e-01  3.51242065e-01\n",
      "  -2.32987672e-01 -3.68500859e-01]\n",
      " [ 1.93766862e-01 -5.31416796e-02 -2.72572041e-01  4.69015628e-01\n",
      "  -2.64070153e-01 -3.71246099e-01]\n",
      " [ 1.62094504e-01 -3.84323969e-02 -1.73719436e-01  4.37080085e-01\n",
      "  -3.50687742e-01 -2.71931171e-01]\n",
      " [ 9.21378732e-02 -1.31258056e-01 -5.98185360e-02  5.61012924e-01\n",
      "  -2.11838603e-01 -3.00779134e-01]\n",
      " [ 2.41735727e-01 -5.80792129e-02 -2.25827187e-01  4.17616934e-01\n",
      "  -2.68126756e-01 -2.56668925e-01]\n",
      " [ 3.10918301e-01  3.06230187e-02 -3.74714494e-01  2.97652543e-01\n",
      "  -2.14250088e-01 -4.03152019e-01]\n",
      " [ 1.20170899e-01 -4.60828543e-02 -3.99853140e-02  5.87423205e-01\n",
      "  -3.55998218e-01 -2.34191686e-01]\n",
      " [ 1.29682466e-01 -1.05687164e-01 -7.26484209e-02  5.33048749e-01\n",
      "  -3.07476699e-01 -2.60542870e-01]\n",
      " [ 1.61407560e-01 -1.00118175e-01 -9.46546048e-02  4.43842888e-01\n",
      "  -2.28855774e-01 -3.30034614e-01]\n",
      " [ 1.26750737e-01 -1.14964440e-01 -7.17296600e-02  4.99428689e-01\n",
      "  -2.38872737e-01 -2.92566180e-01]\n",
      " [ 3.00248444e-01  5.64714819e-02 -2.90967703e-01  3.56258035e-01\n",
      "  -2.70536304e-01 -3.59732419e-01]\n",
      " [ 2.14446053e-01  2.49385834e-03 -2.00139046e-01  4.01269227e-01\n",
      "  -3.15434277e-01 -2.43069366e-01]\n",
      " [ 3.65303844e-01  2.39009857e-02 -3.65291595e-01  3.62002522e-01\n",
      "  -2.35626370e-01 -3.74157667e-01]\n",
      " [ 2.26170450e-01 -3.92764546e-02 -2.06928253e-01  4.42241520e-01\n",
      "  -1.97963879e-01 -2.87687302e-01]\n",
      " [-3.59332561e-03 -2.47833088e-01 -2.33503878e-02  5.83545387e-01\n",
      "  -3.41830343e-01 -2.24235266e-01]\n",
      " [ 2.88938940e-01  1.50570646e-04 -1.69530988e-01  4.33275819e-01\n",
      "  -2.55819023e-01 -2.90759385e-01]\n",
      " [ 3.55652928e-01  3.33491787e-02 -4.41212535e-01  2.15319291e-01\n",
      "  -2.19355911e-01 -4.23694253e-01]\n",
      " [ 3.16310346e-01 -6.41667843e-03 -3.61924231e-01  2.46463507e-01\n",
      "  -2.08173513e-01 -4.27315176e-01]\n",
      " [ 3.25584382e-01 -1.57911703e-03 -3.62892151e-01  2.60866553e-01\n",
      "  -1.39415234e-01 -3.75482202e-01]\n",
      " [-9.43908095e-02 -1.27213836e-01  1.95383579e-02  6.35769606e-01\n",
      "  -3.92394155e-01 -2.14336947e-01]\n",
      " [ 6.97806627e-02 -1.14700876e-01 -6.72167093e-02  4.77355212e-01\n",
      "  -3.52820367e-01 -2.62673080e-01]\n",
      " [ 8.56778622e-02 -8.97206292e-02 -8.90543908e-02  5.40860295e-01\n",
      "  -3.11001778e-01 -2.96770692e-01]\n",
      " [ 1.45966917e-01 -7.71247670e-02 -1.08498022e-01  5.64269304e-01\n",
      "  -3.12859595e-01 -2.60077596e-01]\n",
      " [-8.49796087e-02 -1.31669894e-01  1.16492122e-01  6.88958287e-01\n",
      "  -4.38919663e-01 -1.44680008e-01]\n",
      " [ 3.13136995e-01  5.38715832e-02 -3.32125723e-01  3.33318055e-01\n",
      "  -2.50417471e-01 -3.92377734e-01]\n",
      " [ 1.33346960e-01  1.14147030e-02 -1.98212922e-01  4.40399468e-01\n",
      "  -3.29300284e-01 -2.48120487e-01]\n",
      " [-1.56933367e-02 -1.03670679e-01  2.63085812e-02  6.16872907e-01\n",
      "  -3.88770580e-01 -2.32305691e-01]\n",
      " [ 1.78837493e-01 -8.58823955e-02 -1.69186607e-01  4.78050053e-01\n",
      "  -2.19354659e-01 -3.65279108e-01]\n",
      " [ 1.35449260e-01 -5.80538660e-02 -1.85051918e-01  3.95930946e-01\n",
      "  -3.74634773e-01 -2.52894759e-01]\n",
      " [ 3.83607745e-01  3.72992456e-03 -4.55405325e-01  2.60473430e-01\n",
      "  -2.08466589e-01 -3.97604674e-01]\n",
      " [ 3.25670034e-01  2.49704495e-02 -3.50276351e-01  2.58720964e-01\n",
      "  -2.26012304e-01 -3.81490707e-01]\n",
      " [ 2.47187585e-01 -3.45777124e-02 -2.69203305e-01  3.71207237e-01\n",
      "  -2.01246381e-01 -3.91077995e-01]\n",
      " [ 3.08858931e-01 -5.33795729e-03 -4.31903660e-01  2.50161648e-01\n",
      "  -2.02246994e-01 -4.38009560e-01]\n",
      " [ 2.03565374e-01  7.96833262e-03 -2.17986643e-01  4.16866243e-01\n",
      "  -3.39401245e-01 -2.70472854e-01]\n",
      " [ 3.22352290e-01  3.17676179e-02 -3.68247628e-01  3.53611469e-01\n",
      "  -1.72831655e-01 -4.29293662e-01]], shape=(100, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_TFhugging = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "\n",
    "outputs = model_TFhugging(model_inputs)\n",
    "print(outputs.logits)\n",
    "\n",
    "# kernel crashes with 20000 samples -> data should be made into a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.24582675 0.17414665 0.11131703 0.2204022  0.13451627 0.11379111]\n",
      " [0.18918979 0.14786765 0.1511424  0.27237973 0.12048641 0.11893395]\n",
      " [0.19895288 0.15358657 0.13900922 0.25538218 0.13297997 0.12008906]\n",
      " [0.21903628 0.17230026 0.12469518 0.2309293  0.13561675 0.11742225]\n",
      " [0.2222584  0.17384025 0.11558393 0.24354999 0.12732285 0.11744449]\n",
      " [0.21135485 0.15880954 0.1225524  0.2501289  0.13846058 0.11869378]\n",
      " [0.19576076 0.14997892 0.14251757 0.2654941  0.12153462 0.12471398]\n",
      " [0.20989755 0.16628605 0.13490888 0.23454799 0.14100786 0.11335171]\n",
      " [0.2547862  0.18148918 0.10887679 0.20287806 0.14160918 0.11036064]\n",
      " [0.1702389  0.15387695 0.15783376 0.28463858 0.10825735 0.12515436]\n",
      " [0.17564364 0.15290438 0.15229082 0.2881763  0.1050859  0.1258989 ]\n",
      " [0.18243852 0.15103796 0.15570231 0.27339303 0.10982525 0.12760302]\n",
      " [0.19000988 0.14489272 0.14519207 0.26702508 0.1194166  0.13346367]\n",
      " [0.22889699 0.17446333 0.12042102 0.22753339 0.13810222 0.11058301]\n",
      " [0.16831738 0.15241136 0.15060644 0.304857   0.10167254 0.12213526]\n",
      " [0.2132095  0.16073498 0.13302307 0.24397355 0.13057998 0.11847898]\n",
      " [0.19635688 0.15984286 0.13835481 0.2638705  0.12386747 0.11770735]\n",
      " [0.17019074 0.14711437 0.15975846 0.2851156  0.10727151 0.13054937]\n",
      " [0.20581701 0.1710121  0.12990731 0.2498378  0.12280163 0.12062416]\n",
      " [0.15840979 0.14183623 0.16246293 0.29423246 0.11155546 0.13150306]\n",
      " [0.22931041 0.18117139 0.1185232  0.23126775 0.12996529 0.10976211]\n",
      " [0.14549018 0.1345426  0.17113699 0.30949566 0.11376961 0.12556486]\n",
      " [0.19781123 0.15130499 0.13103247 0.26355365 0.12418947 0.13210826]\n",
      " [0.19436699 0.14999034 0.13689847 0.27903175 0.12243619 0.11727632]\n",
      " [0.18852203 0.16262203 0.14707284 0.2618193  0.12181915 0.11814463]\n",
      " [0.189289   0.1549317  0.1376928  0.26157847 0.12984137 0.12666672]\n",
      " [0.19521439 0.15308192 0.13944475 0.27276075 0.11292588 0.12657239]\n",
      " [0.16085981 0.14563201 0.15517929 0.3037032  0.10820752 0.12641819]\n",
      " [0.24341506 0.16596313 0.11603084 0.21169825 0.14899649 0.11389615]\n",
      " [0.21240701 0.15821286 0.13584429 0.24856712 0.12841944 0.11654939]\n",
      " [0.25249177 0.17791115 0.10611204 0.2164417  0.1379632  0.10908005]\n",
      " [0.23897782 0.17771295 0.1114459  0.23159204 0.12863912 0.11163212]\n",
      " [0.23607743 0.18304323 0.10770565 0.22710204 0.1318146  0.11425695]\n",
      " [0.24630767 0.17946734 0.10818454 0.21758832 0.13690263 0.1115495 ]\n",
      " [0.22329593 0.1636857  0.12940224 0.23556443 0.12327926 0.12477241]\n",
      " [0.20538774 0.15691753 0.1355873  0.24917497 0.13162684 0.12130556]\n",
      " [0.21959451 0.16952781 0.12266395 0.23315907 0.13138816 0.12366644]\n",
      " [0.2395153  0.1717582  0.11259019 0.22792962 0.13359842 0.11460813]\n",
      " [0.17147106 0.1472908  0.15984248 0.2824734  0.11256357 0.12635861]\n",
      " [0.14512922 0.1366916  0.17603782 0.30538094 0.10258782 0.13417256]\n",
      " [0.17814472 0.15259728 0.15357555 0.27465454 0.10602526 0.13500255]\n",
      " [0.20013705 0.15391657 0.13938133 0.2638714  0.12085045 0.12184317]\n",
      " [0.23395869 0.16729978 0.12071963 0.23385882 0.13160914 0.11255386]\n",
      " [0.20705414 0.1557704  0.13548176 0.24741869 0.12923066 0.12504438]\n",
      " [0.21929504 0.16452716 0.12298555 0.24484855 0.13126409 0.11707956]\n",
      " [0.18841219 0.15068041 0.15228443 0.26670322 0.1172156  0.12470422]\n",
      " [0.26593277 0.18562639 0.10357933 0.19556887 0.13462082 0.11467176]\n",
      " [0.1992957  0.1552156  0.13810372 0.27527633 0.11253836 0.11957023]\n",
      " [0.24133238 0.17859472 0.11379629 0.21165709 0.14193816 0.11268141]\n",
      " [0.22846211 0.16578326 0.1275396  0.22828047 0.13711117 0.11282343]\n",
      " [0.24864556 0.17228715 0.11178292 0.2177087  0.13659233 0.11298326]\n",
      " [0.195231   0.16453281 0.13185848 0.26729792 0.11753781 0.12354202]\n",
      " [0.2629238  0.1771329  0.10311045 0.20513909 0.13841017 0.1132836 ]\n",
      " [0.23168072 0.17713255 0.11567706 0.22714607 0.13063341 0.11773003]\n",
      " [0.15477516 0.13996908 0.17533986 0.2849476  0.11269552 0.13227282]\n",
      " [0.24649651 0.1784784  0.10801003 0.21197315 0.13950871 0.11553329]\n",
      " [0.20184697 0.1554596  0.1421011  0.2527028  0.1171766  0.1307129 ]\n",
      " [0.1998485  0.15641046 0.13886663 0.26590985 0.11934034 0.11962412]\n",
      " [0.16225344 0.14817834 0.16021045 0.29256418 0.11016499 0.12662874]\n",
      " [0.22399324 0.17373161 0.12448107 0.23137821 0.1288978  0.11751815]\n",
      " [0.24579369 0.16342984 0.11656778 0.21260314 0.14790767 0.11369793]\n",
      " [0.1719352  0.15409465 0.15236136 0.28634107 0.10460703 0.13066067]\n",
      " [0.23499744 0.17781325 0.11062855 0.22759973 0.13377848 0.1151825 ]\n",
      " [0.18973255 0.15645908 0.14354748 0.27055323 0.11109687 0.12861072]\n",
      " [0.2356903  0.17089826 0.11434007 0.23350352 0.13262913 0.11293882]\n",
      " [0.21529102 0.17245354 0.12387051 0.23888768 0.13318829 0.11630902]\n",
      " [0.20298931 0.15857771 0.12733412 0.26730815 0.1284213  0.11536954]\n",
      " [0.1962212  0.16056769 0.14025028 0.25832748 0.11750258 0.12713085]\n",
      " [0.17636739 0.1410583  0.15150413 0.28186953 0.13013765 0.11906293]\n",
      " [0.20973785 0.15540639 0.13140647 0.25006968 0.12596396 0.12741555]\n",
      " [0.23109329 0.174605   0.11641817 0.2280479  0.13668145 0.11315417]\n",
      " [0.17802384 0.15075615 0.15167822 0.28405553 0.110581   0.12490518]\n",
      " [0.18427299 0.14562702 0.15051873 0.27582997 0.11901615 0.12473518]\n",
      " [0.19383796 0.14923121 0.15004878 0.25709814 0.13120475 0.11857912]\n",
      " [0.18490672 0.1452035  0.15161903 0.2684137  0.12828162 0.12157539]\n",
      " [0.22337048 0.17504704 0.12366986 0.23623838 0.12622261 0.11545154]\n",
      " [0.20422493 0.16521867 0.13491392 0.24617563 0.1202222  0.1292446 ]\n",
      " [0.23726082 0.16863847 0.11427005 0.23647884 0.13009043 0.11326141]\n",
      " [0.20370458 0.15621352 0.1321017  0.2528362  0.13329123 0.1218527 ]\n",
      " [0.16453859 0.12888305 0.16131969 0.29597762 0.11732034 0.13196057]\n",
      " [0.21382397 0.16019066 0.13519026 0.2470251  0.12401407 0.11975581]\n",
      " [0.24596065 0.17819309 0.11086421 0.21375658 0.13840203 0.11282347]\n",
      " [0.23632461 0.17113955 0.11993761 0.22038135 0.13987125 0.11234571]\n",
      " [0.23338148 0.16826008 0.11723704 0.21875592 0.14659515 0.11577027]\n",
      " [0.147209   0.1424556  0.16497311 0.30551955 0.10927307 0.1305697 ]\n",
      " [0.1792379  0.14904265 0.15629052 0.2694246  0.11746177 0.12854259]\n",
      " [0.17825301 0.14957613 0.14967579 0.2810089  0.11988396 0.12160224]\n",
      " [0.18542355 0.14834651 0.14376466 0.28172883 0.1171923  0.1235441 ]\n",
      " [0.14306486 0.13653867 0.17499718 0.31020582 0.10041966 0.13477378]\n",
      " [0.22833312 0.17618598 0.11976636 0.23298793 0.12996317 0.11276329]\n",
      " [0.18957716 0.16781527 0.13607909 0.25771353 0.11936057 0.12945439]\n",
      " [0.15785402 0.14455982 0.1646254  0.29714996 0.10870018 0.12711069]\n",
      " [0.19707593 0.15124018 0.13915174 0.2658153  0.13234298 0.11437386]\n",
      " [0.19508268 0.16076127 0.1415881  0.2531305  0.11713643 0.13230096]\n",
      " [0.24929847 0.17050645 0.10773104 0.22041596 0.13790658 0.11414144]\n",
      " [0.23511487 0.17405556 0.11959701 0.21988949 0.13542147 0.11592153]\n",
      " [0.21505405 0.16224769 0.12831633 0.24344938 0.13733941 0.11359322]\n",
      " [0.23675376 0.17291908 0.11287241 0.22325696 0.14201245 0.11218532]\n",
      " [0.20326716 0.16715544 0.13334891 0.25159535 0.11810269 0.12653045]\n",
      " [0.22925773 0.17144494 0.11492123 0.23653732 0.13972314 0.10811558]], shape=(100, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0',\n",
       " 1: 'LABEL_1',\n",
       " 2: 'LABEL_2',\n",
       " 3: 'LABEL_3',\n",
       " 4: 'LABEL_4',\n",
       " 5: 'LABEL_5'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_TFhugging.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "from keras_nlp.models import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "classifier_keras_nlp = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_base_en\",\n",
    "    num_classes=6,\n",
    "    preprocessor=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bert_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"bert_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ bert_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)                  │ {sequence_output: (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>),   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">108,310,272</span> │\n",
       "│                                               │ pooled_output: (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)}            │                 │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ dropout_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,614</span> │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ bert_backbone (\u001b[38;5;33mBertBackbone\u001b[0m)                  │ {sequence_output: (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m),   │     \u001b[38;5;34m108,310,272\u001b[0m │\n",
       "│                                               │ pooled_output: (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)}            │                 │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ dropout_76 (\u001b[38;5;33mDropout\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                              │           \u001b[38;5;34m4,614\u001b[0m │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">108,314,886</span> (413.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m108,314,886\u001b[0m (413.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">108,314,886</span> (413.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m108,314,886\u001b[0m (413.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_keras_nlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_keras_nlp.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    jit_compile=True,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "# Access backbone programmatically (e.g., to change `trainable`).\n",
    "classifier_keras_nlp.backbone.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This code results in tokenized sequences but gives only 'token_ids'\n",
    "# tokenizer = BertTokenizer.from_preset('bert_base_en')\n",
    "\n",
    "# max_length = 40\n",
    "\n",
    "# X_train_tokenized = tokenizer(X_train.tolist())\n",
    "# X_val_tokenized = tokenizer(X_val.tolist())\n",
    "# X_test_tokenized = tokenizer(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer returning 'token_ids', 'segment_ids', 'padding_mask'\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset('bert_base_en')\n",
    "\n",
    "X_train_preprocessed = preprocessor(X_train.tolist())\n",
    "X_val_preprocessed = preprocessor(X_val.tolist())\n",
    "X_test_preprocessed = preprocessor(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': <tf.Tensor: shape=(1000, 512), dtype=int32, numpy=\n",
       " array([[  101,  1631,  2816, ...,     0,     0,     0],\n",
       "        [  101,  1631,  2489, ...,     0,     0,     0],\n",
       "        [  101,  1253,  1631, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  1631,  1176, ...,     0,     0,     0],\n",
       "        [  101,  1631,  1463, ...,     0,     0,     0],\n",
       "        [  101, 13280, 25906, ...,     0,     0,     0]], dtype=int32)>,\n",
       " 'segment_ids': <tf.Tensor: shape=(1000, 512), dtype=int32, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
       " 'padding_mask': <tf.Tensor: shape=(1000, 512), dtype=bool, numpy=\n",
       " array([[ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False]])>}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier_keras_nlp.fit(X_train_preprocessed, y_train, epochs=5, validation_data=(X_val_preprocessed, y_val), verbose=1)\n",
    "train_loss, train_accuracy, val_loss, val_accuracy = history.history.values()\n",
    "test_loss, test_accuracy = classifier_keras_nlp.evaluate(X_test_preprocessed, y_test, verbose=2)\n",
    "plot_training_history(train_loss, train_accuracy, val_loss, val_accuracy, test_loss, test_accuracy)\n",
    "\n",
    "# kernel crashes at too many samples -> data into datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
